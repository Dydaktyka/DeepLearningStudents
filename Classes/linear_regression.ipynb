{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will consider a simple linear regression model: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ y_i = x_{ij} w_j + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the \"sumation conventions\": when an index is repeated the sumation over this index is implied:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "x_{ij} w_j \\equiv   \\sum_j x_{ij} w_j \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement function `linear(x,w,b)` that given feature matrix $\\mathbf{x}$, weights $\\mathbf{w}$ and bias $b$  returns $\\mathbf{y}$. **Hint** Use matrix multiplication operator `@`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Generate a random feature matrix $\\mathbf{x}$ witch 10000 samples and three features,  \n",
    "such that first feature is drawn from N(0,1), second feature from  U(,1) and third from N(1,2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$N(\\mu,\\sigma)$ denotes normal distribution with mean $\\mu$ and standard deviation $\\sigma$. To generate random numbers you can use `numpy.random.normal` and `numpy.random.uniform` functions. To collect all features together you can use `numpy.stack` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Then using $\\mathbf{x}$, weights $w_{true}$  and  bias $b_{true}$:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "w_true = np.array([0.2, 0.5,-0.2])\n",
    "b_true = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "generate output $\\mathbf{y}$ assuming a normaly distributed $N(0,0.1)$ noise $\\mathbf{\\epsilon}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$ y_i =  \n",
    "x_{ij} w_j+b +\\epsilon_i \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the means square loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ MSE(w,b|y,x) = \\frac{1}{2}\\frac{1}{N}\\sum_{i=0}^{N-1} (y_i -  x_{ij} w_j -b  )^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write down the python function `mse(y,x,w,b)` implementing it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "Find the gradient of the loss function with respect to weights $w$ and bias $b$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve numerically the equations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial}{\\partial w_k} MSE(w,b|y,x) = 0,\\quad \\frac{\\partial}{\\partial b} MSE(w,b|y,x) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for $w_k$ and $b$.  You can use the `numpy.linalg.solve` to solve the linear equation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "answer"
    ]
   },
   "outputs": [],
   "source": [
    "x_bar = x.mean(0)\n",
    "y_bar = y.mean()\n",
    "X = np.transpose(x) @ x / len(x)\n",
    "Y = y @ x /len(y)\n",
    "A = X - np.multiply.outer(x_bar, x_bar)\n",
    "w = np.linalg.solve(A,Y-y_bar*x_bar)\n",
    "b = y_bar - x_bar@w\n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement functions `grad_w(y,x,w,b)` and `grad_b(y,x,w,b)` implementing those gradients.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement gradient descent for linear regression. Starting from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.asarray([0.0,0.0,0.0], dtype='float64')\n",
    "b = 1.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many epochs did you need to get MSE below 0.0075 ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement stochastic gradient descent (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.asarray([0.0,0.0,0.0], dtype='float64')\n",
    "b = 1.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from same parameters as above and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how many epochs did you need to get MSE below 0.0075 ?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement SGD using pytorch. Start by just rewritting Problem 3 to use torch Tensors instead of numpy arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert frrom numpy arrays to torch tensors you can use ``torch.from_numpy()`` function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_y = t.from_numpy(y)\n",
    "t_x = t.from_numpy(x)\n",
    "t_w = t.DoubleTensor([0,0,0])\n",
    "t_b = t.DoubleTensor([1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement SGD using pytorch automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end the variable with respect to which the gradient will be calculated, ``t_w`` in this case, must have attribute\n",
    "``requires_grad`` set to ``True`` (``t_w.require_grad=True``)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torch will automatically track any expression containing ``t_w`` and store its computational graph. The method ``backward()`` can be run on the final expression to back propagate the gradient e.g. ``loss.backward()``. Then the gradient is accesible as ``t_w.grad``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_y = t.from_numpy(y)\n",
    "t_x = t.from_numpy(x)\n",
    "t_w = t.DoubleTensor([0,0,0])\n",
    "t_b = t.DoubleTensor([1.0])\n",
    "t_w.requires_grad_(True)\n",
    "t_b.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 9 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement SGD using pytorch  optimisers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_y = t.from_numpy(y)\n",
    "t_x = t.from_numpy(x)\n",
    "t_w = t.DoubleTensor([0,0,0])\n",
    "t_b = t.DoubleTensor([1.0])\n",
    "t_w.requires_grad_(True)\n",
    "t_b.requires_grad_(True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
