{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import gridspec\n",
    "from scipy.stats  import norm, multivariate_normal\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Negative Logarithmic Likelihood loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$-\\sum_{i}\\sum_{j} l_{ij} \\log \\tilde{P}(C_j|x_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is an approximation of the quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ -\\int \\text{d}X  P(X) \\sum_j P(C_j|X)\\log \\tilde{P}(C_j|X) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_j P(C_j|X)\\log \\tilde{P}(C_j|X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is the [_cross entropy_ ](https://en.wikipedia.org/wiki/Cross_entropy) between distributions $P(C_j|X)$: the true distribution of categories $C_i$ given features $X$ and approximating distribution $ \\tilde{P}(C_j|X)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy between two discrete distributions $p$ and $q$ is defined as the expactaion value of $\\log q$ with respect to $p$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H(p,q)\\equiv -E[\\log q]_p =  -\\sum p_i \\log q_i$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can show that this quantity is minimised if and only if $p_i=q_i$. Cross entropy has several properties that make it suitable loss function for working with neural networks. We will explore this below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a simple binary case with two categories 0 and 1. Let the true probability of 1 be $p$ and approximated probability be denoted $\\tilde{p}$. We can look for $\\tilde{p}$ by minimizing the _binary cross entropy_: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ \\operatorname{BCE}(\\tilde p, p) = - p\\log \\tilde{p} - (1-p) \\log (1-\\tilde{p})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "__Problem__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "problem"
    ]
   },
   "source": [
    "Show that this function as a function of $\\tilde{p}$ does have a minimum when $\\tilde{p}=p$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider a simple case when the probability $\\tilde p$ is given by simple logistic function of some parameter $x$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\tilde{p} = \\frac{1}{1+e^{-x}}, \\quad 1-\\tilde{p} = \\frac{1}{1+e^x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then cross entropy is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$p \\log (1+e^{-x}) +  (1-p) \\log (1+e^x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and its  derivative by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\frac{\\text{d}}{\\text{d}x}BCE(\\tilde{p}(x),p)=-\\frac{p}{1+e^{-x}}+\\frac{1-p}{1+e^x} = \\frac{1}{1+e^{-x}}-p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the MSE error this is respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$MSE(x,p) = \\frac{1}{2}\\left(\\tilde{p}-p\\right)^2 =  \\frac{1}{2}\\left(\\frac{1}{1+e^{-x}}-p\\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\frac{\\text{d}}{\\text{d}x}MSE(\\tilde{p}(x),p)=-\\left(\\frac{1}{1+e^{-x}}-p\\right) \\frac{e^{-x}}{\\left(1+e^{-x}\\right)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce(pt,p):\n",
    "    return -p*np.log(pt)+ -(1-p)*np.log(1-pt)\n",
    "\n",
    "def mse(pt,p):\n",
    "    return 0.5*(p-pt)*(p-pt)\n",
    "\n",
    "def logistic(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def logit(p):\n",
    "    return np.log(p/(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "xs=np.linspace(-10,10,400)\n",
    "p=0.9\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
    "\n",
    "ax[0].set_title('Cross entropy')\n",
    "ax[0].plot(xs,bce(logistic(xs),p))\n",
    "ax[0].axhline(0,c='black');\n",
    "ax[0].axvline(logit(p), c='red');\n",
    "\n",
    "ax[1].set_title('MSE')\n",
    "ax[1].plot(xs,mse(logistic(xs),p));\n",
    "ax[1].axhline(0,c='black');\n",
    "ax[1].axvline(logit(p),c='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see from those plots is that the cross  entropy functions, contrary to MSE does not saturatefor large positive and negative values of parameter $x$. Actually it's behaviour is asymptoticaly  linear. That means that it will have non-zero gradients, while MSE gradients will be zero. This is verified by the derivative plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ys=-(1.0/(1+np.exp(-xs))-p)*np.exp(-xs)/(1+np.exp(-xs))**2\n",
    "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
    "\n",
    "ax[0].set_title('Cross entropy')\n",
    "ys=-(logistic(xs)-p)\n",
    "ax[0].plot(xs,ys)\n",
    "ax[0].axvline(logit(p),c='red');\n",
    "ax[0].axhline(0,c='black');\n",
    "\n",
    "ax[1].set_title('MSE')\n",
    "ax[1].plot(xs,ys*np.exp(-xs)*logistic(xs)**2)\n",
    "ax[1].axvline(np.log(p)-np.log(1-p),c='red');\n",
    "ax[1].axhline(0,c='black');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this toy example is nor realy an example of machine learning, similar behaviour persists in more realistic scenarios. Consider a problem of separating two samples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x1 =  multivariate_normal((7,7),(1,1)).rvs(size=100)\n",
    "x2 = multivariate_normal((-7,-7), (1,1)).rvs(size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((x1,x2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.concatenate((np.ones(100), np.zeros(100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cols=np.array(['red','blue'])\n",
    "fig,ax = plt.subplots(figsize=(8,8))\n",
    "ax.scatter(X[:,0],X[:,1],c=cols[Y.astype(np.int)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use logistic regression for this task:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\tilde{y}_i = \\beta_0x_{i0} +\\beta_1x_{i1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ \\tilde{p}_i = \\frac{1}{1+e^{-y_i}}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def lin(x,b1,b2):\n",
    "    return np.moveaxis(np.multiply.outer(x[:,0],b1) +  np.multiply.outer(x[:,1],b2),0,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def logistic(x,b1,b2):\n",
    "    logit = lin(x,b1,b2)\n",
    "    return 1/(1+np.exp(-logit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Means squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will plot the loss function with MSE error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\frac{1}{2}\\sum_i (\\tilde{p}_i-l_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def mse(x, y, b1, b2):\n",
    "        err = logistic(x,b1,b2)-y\n",
    "        return 0.5*np.sum(err*err, axis=-1)/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "b1s = np.linspace(-2,2,500)\n",
    "b2s = np.linspace(-2,2,500)\n",
    "grid  = np.meshgrid(b1s,b2s)\n",
    "zs = mse(X,Y, grid[0], grid[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,8))\n",
    "gs=gridspec.GridSpec(1,2, width_ratios=[4,0.2])\n",
    "ax1 = plt.subplot(gs[0])\n",
    "ax2 = plt.subplot(gs[1])\n",
    "cs=ax1.contourf(grid[0], grid[1],zs, levels=40);\n",
    "ax1.plot([-2,2],[-2,2],c='red', linewidth=1, linestyle='--')\n",
    "fig.colorbar(cs, cax=ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the values along the diagonal (red) line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "phis = np.linspace(-2,2,500)\n",
    "es = mse(X,Y, phis, phis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "minimize(lambda x: mse(X,Y,x,x),[-2]).x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(phis,es)\n",
    "plt.axvline(1,c='green');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see a \"plateaux\" on both sides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will plot the binary cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\sum_i l_i \\log \\tilde{p}_i + (1-l_i)\\log (1-\\tilde{p})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def ce(x, y, b1, b2):\n",
    "        logit =  logistic(x,b1,b2)\n",
    "        return -np.sum(y*np.log(logit) + (1-y)*np.log(1-logit), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ce(X,Y,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ces = ce(X,Y, grid[0], grid[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,8))\n",
    "gs=gridspec.GridSpec(1,2, width_ratios=[4,0.2])\n",
    "ax1 = plt.subplot(gs[0])\n",
    "ax2 = plt.subplot(gs[1])\n",
    "cs=ax1.contourf(grid[0], grid[1],ces, levels=40);\n",
    "ax1.plot([-2,2],[-2,2],c='red', linewidth=1, linestyle='--')\n",
    "fig.colorbar(cs, cax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "phis = np.linspace(-2,2,500)\n",
    "es = ce(X,Y,phis, phis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(phis,es)\n",
    "plt.axvline(np.pi/4,c='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the \"plateaux\" is only on one side. But this is the right side! When we are there the value of error is already low. Contrary, when the loss is big we are on the slope with non-zero gradient. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "jupytext": {
   "cell_metadata_json": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
